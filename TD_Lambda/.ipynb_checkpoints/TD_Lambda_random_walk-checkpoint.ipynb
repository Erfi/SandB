{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor:       Erfan Azad (erfan@dartmouth.edu)\\nDate:         25 January 2017\\nDescription:  Simulation for TD-Lambda algorithm for a\\n              random walk in n states  \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author:       Erfan Azad (erfan@dartmouth.edu)\n",
    "Date:         25 January 2017\n",
    "Description:  Simulation for TD-Lambda algorithm for a\n",
    "              random walk in n states  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildEpisode(numStates):\n",
    "    \"\"\"\n",
    "    Builds an episode of the TD_Lambda\n",
    "    simulation. Starting form the middle state\n",
    "\n",
    "    Args:\n",
    "        numStates: Number of states.\n",
    "\n",
    "    Returns:\n",
    "        An array of [visitedRewards, visitedStates], which\n",
    "        contain the rewards achieved and states\n",
    "        that were visited.\n",
    "    \"\"\"\n",
    "    assert(numStates > 2)\n",
    "    st = numStates/2     # start from the middle state\n",
    "    rew = 0\n",
    "    visitedStates = [st]\n",
    "    visitedRewards = []\n",
    "    while(st > 0 and st < numStates-1):\n",
    "        if(np.random.random() > 0.5):\n",
    "            st += 1\n",
    "        else:\n",
    "            st -= 1\n",
    "        if(st != 0 and st != numStates-1):\n",
    "            rew = 0\n",
    "        elif(st == 0):\n",
    "            rew = -1\n",
    "        else:\n",
    "            rew = 1\n",
    "        visitedStates.append(st)\n",
    "        visitedRewards.append(rew)\n",
    "    return [visitedStates, visitedRewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TDL_offline_learn_forward(lambda_factor, gamma, alpha, numStates, numEpisodes):\n",
    "    \"\"\"\n",
    "    Runs the TD-Lambda learning algorithm for \n",
    "    the random walk problem. (forward view)\n",
    "    \n",
    "    Args:\n",
    "        lambda_factor:      decay factor (\"lambda\" is a keyword in python!)\n",
    "        gamma:       discount factor\n",
    "        alpha:       learning rate\n",
    "        numStates:   number of states (including the terminal states)\n",
    "        numEpisodes: number of times/ episodes to repeat the learning\n",
    "    \n",
    "    Returns:\n",
    "        The learned values of each state: V(s)\n",
    "    \"\"\"\n",
    "    V = np.zeros((1,numStates))\n",
    "    for i in range(numEpisodes):\n",
    "        visitedStates ,visitedRewards = buildEpisode(numStates)\n",
    "        T = len(visitedRewards) # Final time index\n",
    "        deltaV = np.zeros((1,numStates))\n",
    "        for t in range(0,T):\n",
    "            Rtn = np.zeros((1,T-t))\n",
    "            for n in range(1,T-t+1):\n",
    "                max_n = min(n, T-t)\n",
    "#                 print(\"max_n:{}, t:{}, n:{}, T:{}\".format(max_n, t, n, T))\n",
    "                gammaPowers = np.arange(0,max_n)\n",
    "                Rs = np.array(visitedRewards[t:t+max_n]) #rewards needed to built Rtn with current t and n\n",
    "                Rtn[0,n-1] = np.dot(np.power(gamma, gammaPowers), Rs) + pow(gamma,max_n)*V[0,visitedStates[t+max_n]]\n",
    "            lambdaPowers = np.arange(0,Rtn.shape[1])\n",
    "            Rtl = (1-lambda_factor)*np.dot(np.power(lambda_factor, lambdaPowers)[0:-1], Rtn[0,0:-1]) + pow(lambda_factor,lambdaPowers[-1])*Rtn[0,-1]   \n",
    "            # off-line algorithm --> save the deltas and add at the end of the episode\n",
    "            deltaV[0,visitedStates[t]] += alpha*(Rtl - V[0,visitedStates[t]])\n",
    "        V = V + deltaV\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.54813698 -0.06276626  0.37311259  0.        ]]\n",
      "Performance Time: 3.95213389397 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "stateValues = TDL_offline_learn_forward(0.5,0.9,0.1,5,10000)\n",
    "t2 = time.time()\n",
    "print(stateValues)\n",
    "print(\"Performance Time: {} seconds\".format((t2-t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TDL_offline_learn_backward(lambda_factor, gamma, alpha, numStates, numEpisodes):\n",
    "    \"\"\"\n",
    "    Runs the TD-Lambda learning algorithm for \n",
    "    the random walk problem. (Backward View)\n",
    "    \n",
    "    Args:\n",
    "        lambda_factor:      decay factor (\"lambda\" is a keyword in python!)\n",
    "        gamma:       discount factor\n",
    "        alpha:       learning rate\n",
    "        numStates:   number of states (including the terminal states)\n",
    "        numEpisodes: number of times/ episodes to repeat the learning\n",
    "    \n",
    "    Returns:\n",
    "        The learned values of each state: V(s)\n",
    "    \"\"\"\n",
    "    V = np.zeros((1,numStates))\n",
    "    e = np.zeros((1,numStates))\n",
    "    for i in range(numEpisodes):\n",
    "        visitedStates ,visitedRewards = buildEpisode(numStates)\n",
    "        deltaV = np.zeros((1,numStates))\n",
    "        T = len(visitedRewards)\n",
    "        err = 0\n",
    "        for t in range(0,T):\n",
    "            err = visitedRewards[t] + gamma*V[0,visitedStates[t+1]] - V[0,visitedStates[t]]\n",
    "            e[0,visitedStates[t]] += 1\n",
    "            \n",
    "            deltaV = deltaV + alpha*err*e\n",
    "            e = gamma*lambda_factor*e\n",
    "        V = V + deltaV\n",
    "    return V\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.46342452 -0.00082637  0.56701469  0.        ]]\n",
      "Performance Time: 0.68082690239 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "stateValues = TDL_offline_learn_backward(0.5,0.9,0.1,5,10000)\n",
    "t2 = time.time()\n",
    "print(stateValues)\n",
    "print(\"Performance Time: {} seconds\".format((t2-t1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
