{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor:       Erfan Azad (erfan@dartmouth.edu)\\nDate:         01 February 2017\\nDescription:  Simulation for SARSA-Lambda algorithm for a\\n              random walk in n states  \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author:       Erfan Azad (erfan@dartmouth.edu)\n",
    "Date:         01 February 2017\n",
    "Description:  Simulation for SARSA-Lambda algorithm for a\n",
    "              random walk in n states  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_eps_greedy_action(state_actions, currentState, eps):\n",
    "    '''\n",
    "    Recieves a list of states and their actions, \n",
    "    and selects an action in a eps-greedy fashion\n",
    "    for the current state using the given epsilon.\n",
    "    \n",
    "    Args:\n",
    "        state_actions: nd-array of states and their action values.\n",
    "        \n",
    "        currentState: The state that the action will be selected for.\n",
    "        \n",
    "        eps: Epsilon variable used in epsilon-greedy action selection.\n",
    "        \n",
    "    Returns:\n",
    "        The index of the action-value corresponding to an action.\n",
    "        e.g. 1,2,.. (Note: 0 is the index for the state itself)\n",
    "    '''\n",
    "    assert (currentState > 0 and currentState < state_actions.shape[0]-1)\n",
    "    if (np.random.random() > eps):\n",
    "        action = np.argmax(state_actions[currentState,1:]) + 1 # column of the Q corresponding to the greedy action in the state_actions\n",
    "    else:\n",
    "        action = np.random.choice([1,2])  # ACTION 1 or ACTION 2\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_reward(st, numStates):\n",
    "    '''\n",
    "    Calculates the reward for the 1-D random walk problem.\n",
    "    \n",
    "    Args:\n",
    "        st: The state that the agent is heading to.\n",
    "        \n",
    "        numStates: Total number of states including the terminal\n",
    "                states.\n",
    "    \n",
    "    Returns:\n",
    "        The reward of\n",
    "        -1 for state 0,\n",
    "        +1 for the right most state (index = numStates - 1)\n",
    "        0 for any other state in between.\n",
    "    '''\n",
    "    reward = None\n",
    "    if (st != 0 and st != numStates-1):\n",
    "        reward = 0\n",
    "    elif (st == 0):\n",
    "        reward = -1\n",
    "    else:\n",
    "        reward = 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeStep(state_actions, currentState, numStates, epsilon):\n",
    "    '''\n",
    "    Takes a step in the episode.\n",
    "    \n",
    "    Args:\n",
    "        state_actions: a (numStates x 3) size array representing\n",
    "                    each state s and its action-values:  Q(s,a)\n",
    "        \n",
    "        currentState: current state that we are taking the step from.\n",
    "        \n",
    "        epsilon: epsilon variable used in the epsilon-greedy action selection.\n",
    "        \n",
    "    Returns: \n",
    "        Action taken, observed reward, and the next state in form of [a, r, s_next]\n",
    "    '''\n",
    "    action = choose_eps_greedy_action(state_actions, currentState, epsilon) # Choose and action in current state \n",
    "    next_st = (currentState + 1 if action==2 else currentState -1)          # Observe next state\n",
    "    reward = calculate_reward(next_st, numStates)                           # Observe the reward\n",
    "    \n",
    "    return [action, reward, next_st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildEpisode_SARSA(state_actions, numStates, epsilon):\n",
    "    '''\n",
    "    Builds an episode for SARSA_Lambda for\n",
    "    a 1-D random walk problem.\n",
    "    \n",
    "    Args:\n",
    "        state_actions: a (numStates x 3) size array representing\n",
    "                    each state s and its action-values:  Q(s,a)\n",
    "        \n",
    "        numStates: Number of states including the two\n",
    "                terminal states.\n",
    "                \n",
    "        epsilon: epsilon that is used in the eps-greedy\n",
    "                action selection method. determins the \n",
    "                extent of taking explorative actions.\n",
    "                e.g. epsilon=0.1 --> 10% of the time \n",
    "    \n",
    "    returns:\n",
    "        And array containing state, action, reward history\n",
    "        e.g. [[s0,a0,r0], [s1,a1,r1], ...]\n",
    "    '''\n",
    "    assert (numStates > 2)\n",
    "    state_action_reward_history = []\n",
    "    st = numStates/2          # Start from the middle state\n",
    "    reward = None             # Initializing the reward\n",
    "    while (st > 0 and st < numStates-1):\n",
    "        action, reward, next_st = takeStep(state_actions, st, numStates, epsilon)\n",
    "        state_action_reward_history.append([st, action, reward])  # Record [currentState, action, reward]\n",
    "        st = next_st\n",
    "    return state_action_reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SARSA_Lambda_offline(lambda_factor, gamma, alpha, epsilon, numStates, numEpisodes):\n",
    "    \"\"\"\n",
    "    Runs the SARSA-Lambda learning algorithm for \n",
    "    the random walk problem. It used the offline\n",
    "    version such that it will update all the\n",
    "    action-values, Q(s,a), after each episode.\n",
    "    \n",
    "    Args:\n",
    "        lambda_factor:      decay factor (\"lambda\" is a keyword in python!)\n",
    "        gamma:       discount factor\n",
    "        alpha:       learning rate\n",
    "        numStates:   number of states (including the terminal states)\n",
    "        numEpisodes: number of times/ episodes to repeat the learning\n",
    "    \n",
    "    Returns:\n",
    "        The learned values of each state: V(s)\n",
    "    \"\"\"\n",
    "    state_actions = np.vstack((np.arange(numStates), np.zeros((2,numStates)))).T # e.g. row0 ==> [S0,Q1,Q2] for numState rows\n",
    "    e_trace = np.zeros((numStates, 2))\n",
    "    deltaQ = np.zeros((numStates, 2))\n",
    "    for i in range(numEpisodes):\n",
    "        history = buildEpisode_SARSA(state_actions, numStates, epsilon)\n",
    "        T = len(history)\n",
    "        err = 0\n",
    "        for t in range(0,T):\n",
    "            #err = rt + gamma*Q(st+1, ai) - Q(st,ai)\n",
    "            reward = history[t][2]\n",
    "            Q = state_actions[history[t][0], history[t][1]]\n",
    "            try:\n",
    "                Q_next = state_actions[history[t+1][0], history[t+1][1]]\n",
    "            except IndexError:\n",
    "                Q_next = 0\n",
    "            err = reward + gamma*Q_next - Q\n",
    "            e_trace[history[t][0],history[t][1]-1] += 1\n",
    "            \n",
    "            deltaQ = (1-alpha)*deltaQ + alpha*err*e_trace\n",
    "            e_trace = gamma*lambda_factor*e_trace\n",
    "            e_trace[e_trace < 0.0001] = 0\n",
    "        state_actions[:,1:] += deltaQ\n",
    "    return np.round(state_actions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SARSA_Lambda_online(lambda_factor, gamma, alpha, epsilon, numStates, numEpisodes):\n",
    "    \"\"\"\n",
    "    Runs the SARSA-Lambda learning algorithm for \n",
    "    the random walk problem. It used the online\n",
    "    version such that it will update the\n",
    "    action-values, Q(s,a), at each step of the episode.\n",
    "    \n",
    "    Args:\n",
    "        lambda_factor:      decay factor (\"lambda\" is a keyword in python!)\n",
    "        gamma:       discount factor\n",
    "        alpha:       learning rate\n",
    "        numStates:   number of states (including the terminal states)\n",
    "        numEpisodes: number of times/ episodes to repeat the learning\n",
    "    \n",
    "    Returns:\n",
    "        The learned values of each state: V(s)\n",
    "    \"\"\"\n",
    "    state_actions = np.vstack((np.arange(numStates), np.zeros((2,numStates)))).T # e.g. row0 ==> [S0,Q1,Q2] for numState rows\n",
    "    e_trace = np.zeros((numStates, 2))\n",
    "    for i in range(numEpisodes):\n",
    "        currentState = numStates /2 # Start from the middle\n",
    "        while (currentState >0  and currentState < numStates-1):\n",
    "            action, reward, nextState = takeStep(state_actions, currentState, numStates, epsilon) # Take action, observe reward and the nextState\n",
    "            Q = state_actions[currentState, action]\n",
    "            try:\n",
    "                nextAction =  choose_eps_greedy_action(state_actions, nextState, epsilon)         # Choose nextAction from nextState\n",
    "                Q_next = state_actions[nextState, nextAction]\n",
    "            except AssertionError:\n",
    "                Q_next = 0 # nextState is a Terminal State\n",
    "            err = reward + gamma*Q_next - Q\n",
    "            e_trace[currentState, action - 1] += 1\n",
    "            # For all s, a update the Q(s,a) and e_trace(s,a)\n",
    "            state_actions[:,1:] += alpha*err*e_trace\n",
    "            e_trace = gamma*lambda_factor*e_trace\n",
    "            \n",
    "            # Update currentState\n",
    "            currentState = nextState\n",
    "    return np.round(state_actions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00  -1.00000000e-01  -0.00000000e+00]\n",
      " [  2.00000000e+00  -5.00000000e-02  -0.00000000e+00]\n",
      " [  3.00000000e+00  -2.00000000e-02  -0.00000000e+00]\n",
      " [  4.00000000e+00  -1.00000000e-02   1.50000000e-01]\n",
      " [  5.00000000e+00   8.00000000e-02   3.70000000e-01]\n",
      " [  6.00000000e+00   3.00000000e-01   4.20000000e-01]\n",
      " [  7.00000000e+00   3.70000000e-01   4.90000000e-01]\n",
      " [  8.00000000e+00   4.30000000e-01   5.60000000e-01]\n",
      " [  9.00000000e+00   4.70000000e-01   6.30000000e-01]\n",
      " [  1.00000000e+01   5.50000000e-01   7.00000000e-01]\n",
      " [  1.10000000e+01   6.20000000e-01   7.90000000e-01]\n",
      " [  1.20000000e+01   7.00000000e-01   9.00000000e-01]\n",
      " [  1.30000000e+01   8.00000000e-01   1.00000000e+00]\n",
      " [  1.40000000e+01   0.00000000e+00   0.00000000e+00]]\n",
      "Finished in 24.8786411285 seconds.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "result = SARSA_Lambda_offline(lambda_factor=0.5,gamma=0.9, alpha=0.1, epsilon=0.2, numStates=15, numEpisodes=10000)\n",
    "t2 = time.time()\n",
    "print(result)\n",
    "print(\"Finished in {} seconds.\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00  -1.00000000e-01   0.00000000e+00]\n",
      " [  2.00000000e+00  -4.00000000e-02   0.00000000e+00]\n",
      " [  3.00000000e+00  -2.00000000e-02   0.00000000e+00]\n",
      " [  4.00000000e+00  -1.00000000e-02   0.00000000e+00]\n",
      " [  5.00000000e+00  -0.00000000e+00   0.00000000e+00]\n",
      " [  6.00000000e+00  -0.00000000e+00  -0.00000000e+00]\n",
      " [  7.00000000e+00  -0.00000000e+00   0.00000000e+00]\n",
      " [  8.00000000e+00  -0.00000000e+00   0.00000000e+00]\n",
      " [  9.00000000e+00  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+01  -0.00000000e+00  -0.00000000e+00]\n",
      " [  1.10000000e+01  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.20000000e+01  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.30000000e+01  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.40000000e+01  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.50000000e+01  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.60000000e+01  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.70000000e+01  -0.00000000e+00   0.00000000e+00]\n",
      " [  1.80000000e+01   0.00000000e+00   0.00000000e+00]\n",
      " [  1.90000000e+01   0.00000000e+00   3.00000000e-02]\n",
      " [  2.00000000e+01   2.00000000e-02   5.00000000e-02]\n",
      " [  2.10000000e+01   5.00000000e-02   6.00000000e-02]\n",
      " [  2.20000000e+01   5.00000000e-02   7.00000000e-02]\n",
      " [  2.30000000e+01   6.00000000e-02   8.00000000e-02]\n",
      " [  2.40000000e+01   7.00000000e-02   9.00000000e-02]\n",
      " [  2.50000000e+01   7.00000000e-02   1.00000000e-01]\n",
      " [  2.60000000e+01   9.00000000e-02   1.20000000e-01]\n",
      " [  2.70000000e+01   1.00000000e-01   1.30000000e-01]\n",
      " [  2.80000000e+01   1.20000000e-01   1.50000000e-01]\n",
      " [  2.90000000e+01   1.30000000e-01   1.70000000e-01]\n",
      " [  3.00000000e+01   1.50000000e-01   1.90000000e-01]\n",
      " [  3.10000000e+01   1.70000000e-01   2.10000000e-01]\n",
      " [  3.20000000e+01   2.00000000e-01   2.40000000e-01]\n",
      " [  3.30000000e+01   2.20000000e-01   2.70000000e-01]\n",
      " [  3.40000000e+01   2.50000000e-01   3.10000000e-01]\n",
      " [  3.50000000e+01   2.90000000e-01   3.60000000e-01]\n",
      " [  3.60000000e+01   3.10000000e-01   3.90000000e-01]\n",
      " [  3.70000000e+01   3.50000000e-01   4.40000000e-01]\n",
      " [  3.80000000e+01   4.00000000e-01   5.20000000e-01]\n",
      " [  3.90000000e+01   4.80000000e-01   5.80000000e-01]\n",
      " [  4.00000000e+01   5.50000000e-01   6.60000000e-01]\n",
      " [  4.10000000e+01   6.10000000e-01   7.80000000e-01]\n",
      " [  4.20000000e+01   6.70000000e-01   9.00000000e-01]\n",
      " [  4.30000000e+01   7.80000000e-01   1.00000000e+00]\n",
      " [  4.40000000e+01   0.00000000e+00   0.00000000e+00]]\n",
      "Finished in 15.3624470234 seconds.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "result = SARSA_Lambda_online(lambda_factor=0.5,gamma=0.9, alpha=0.1, epsilon=0.2, numStates=45, numEpisodes=10000)\n",
    "t2 = time.time()\n",
    "print(result)\n",
    "print(\"Finished in {} seconds.\".format(t2-t1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
