{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor:       Erfan Azad (erfan@dartmouth.edu)\\nDate:         02 February 2017\\nDescription:  Simulation for SARSA-Lambda algorithm for a\\n              random walk in n states  \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author:       Erfan Azad (erfan@dartmouth.edu)\n",
    "Date:         02 February 2017\n",
    "Description:  Simulation for SARSA-Lambda algorithm for a\n",
    "              random walk in n states  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_eps_greedy_action(state_actions, currentState, eps):\n",
    "    '''\n",
    "    Recieves a list of states and their actions, \n",
    "    and selects an action in a eps-greedy fashion\n",
    "    for the current state using the given epsilon.\n",
    "    \n",
    "    Args:\n",
    "        state_actions: nd-array of states and their action values.\n",
    "        \n",
    "        currentState: The state that the action will be selected for.\n",
    "        \n",
    "        eps: Epsilon variable used in epsilon-greedy action selection.\n",
    "        \n",
    "    Returns:\n",
    "        The index of the action-value corresponding to an action.\n",
    "        e.g. 1,2,.. (Note: 0 is the index for the state itself)\n",
    "    '''\n",
    "    assert (currentState > 0 and currentState < state_actions.shape[0]-1)\n",
    "    if (np.random.random() > eps):\n",
    "        action = np.argmax(state_actions[currentState,1:]) + 1 # column of the Q corresponding to the greedy action in the state_actions\n",
    "    else:\n",
    "        action = np.random.choice([1,2])  # ACTION 1 or ACTION 2\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_reward(st, numStates):\n",
    "    '''\n",
    "    Calculates the reward for the 1-D random walk problem.\n",
    "    \n",
    "    Args:\n",
    "        st: The state that the agent is heading to.\n",
    "        \n",
    "        numStates: Total number of states including the terminal\n",
    "                states.\n",
    "    \n",
    "    Returns:\n",
    "        The reward of\n",
    "        -1 for state 0,\n",
    "        +1 for the right most state (index = numStates - 1)\n",
    "        0 for any other state in between.\n",
    "    '''\n",
    "    reward = None\n",
    "    if (st != 0 and st != numStates-1):\n",
    "        reward = 0\n",
    "    elif (st == 0):\n",
    "        reward = -1\n",
    "    else:\n",
    "        reward = 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeStep(state_actions, currentState, numStates, epsilon):\n",
    "    '''\n",
    "    Takes a step in the episode.\n",
    "    \n",
    "    Args:\n",
    "        state_actions: a (numStates x 3) size array representing\n",
    "                    each state s and its action-values:  Q(s,a)\n",
    "        \n",
    "        currentState: current state that we are taking the step from.\n",
    "        \n",
    "        epsilon: epsilon variable used in the epsilon-greedy action selection.\n",
    "        \n",
    "    Returns: \n",
    "        Action taken, observed reward, and the next state in form of [a, r, s_next]\n",
    "    '''\n",
    "    action = choose_eps_greedy_action(state_actions, currentState, epsilon) # Choose and action in current state \n",
    "    next_st = (currentState + 1 if action==2 else currentState -1)          # Observe next state\n",
    "    reward = calculate_reward(next_st, numStates)                           # Observe the reward\n",
    "    \n",
    "    return [action, reward, next_st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Q_Lambda_Watkins(lambda_factor, gamma, alpha, epsilon, numStates, numEpisodes):\n",
    "    \"\"\"\n",
    "    Runs the Q-Lambda Watkins learning algorithm for \n",
    "    the random walk problem. It used the online\n",
    "    version such that it will update the\n",
    "    action-values, Q(s,a), at each step of the episode.\n",
    "    \n",
    "    Args:\n",
    "        lambda_factor:      decay factor (\"lambda\" is a keyword in python!)\n",
    "        gamma:       discount factor\n",
    "        alpha:       learning rate\n",
    "        numStates:   number of states (including the terminal states)\n",
    "        numEpisodes: number of times/ episodes to repeat the learning\n",
    "    \n",
    "    Returns:\n",
    "        The learned values of each state: V(s)\n",
    "    \"\"\"\n",
    "    state_actions = np.vstack((np.arange(numStates), np.zeros((2,numStates)))).T # e.g. row0 ==> [S0,Q1,Q2] for numState rows\n",
    "    e_trace = np.zeros((numStates, 2))\n",
    "    for i in range(numEpisodes):\n",
    "        currentState = numStates /2 # Start from the middle\n",
    "        while (currentState >0  and currentState < numStates-1):\n",
    "            action, reward, nextState = takeStep(state_actions, currentState, numStates, epsilon) # Take action, observe reward and the nextState\n",
    "            Q = state_actions[currentState, action]\n",
    "            try:\n",
    "                nextAction =  choose_eps_greedy_action(state_actions, nextState, epsilon)         # Choose nextAction from nextState\n",
    "                optimal_nextAction = np.argmax(state_actions[nextState,1:]) + 1\n",
    "                Q_next = state_actions[nextState, optimal_nextAction]\n",
    "            except AssertionError:\n",
    "                Q_next = 0  # nextState is a Terminal State\n",
    "                nextAction = None\n",
    "                optimal_nextAction = None\n",
    "                                               \n",
    "            err = reward + gamma*Q_next - Q\n",
    "            e_trace[currentState, action - 1] += 1\n",
    "                                               \n",
    "            # For all s, a update the Q(s,a) and e_trace(s,a)\n",
    "            print(err)\n",
    "            state_actions[:,1:] += alpha*err*e_trace\n",
    "            if (optimal_nextAction != None and optimal_nextAction == nextAction):                               \n",
    "                e_trace = gamma*lambda_factor*e_trace\n",
    "            else:\n",
    "                e_trace = e_trace = np.zeros((numStates, 2))\n",
    "            # Update currentState\n",
    "            currentState = nextState\n",
    "    return np.round(state_actions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "result = Q_Lambda_Watkins(lambda_factor=0.5,gamma=0.9, alpha=0.1, epsilon=0.2, numStates=45, numEpisodes=500)\n",
    "t2 = time.time()\n",
    "print(result)\n",
    "print(\"Finished in {} seconds.\".format(t2-t1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
